---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a third-year master's student in Zhejiang University now. I also worked at [MSRA-Natural Language Computing Group](https://www.microsoft.com/en-us/research/group/natural-language-computing/) <img src='./images/microsoft_logo.svg' style="width: 4em;">, Alibaba Speech Lab, Qwen LLM Team ago. I have published some papers (Á¨¨‰∏Ä‰ΩúËÄÖ) at the top international AI conferences such as ICLR2025, ACL2025, ACL2024, AAAI2025, ICASSP2024 with total google scholar citations <a href='https://scholar.google.com/citations?user=zuRaB-oAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>. I was selected as Âçé‰∏∫Â§©ÊâçÂ∞ëÂπ¥(‰∏ÄÊ°£), ÈòøÈáåÊòü(ÈõÜÂõ¢) and Chu Kochen Presidential Scholarship(ÊµôÂ§ßÂçÅ‰Ω≥Á†îÁ©∂Áîü) Finalist this year.



<!--

My first-author(include co-first) paper include:
- **Large Audio Languge Models, End-to-End Spoken Dialogue Models.** (WavTokenzier, WavChat, WavRAG, WavReward, WavOmni, WavBench)
- **Text to Speech.** (TextrolSpeech, MobileSpeech, ControlSpeech)
- **Codec.** (Language-Codec, UniCodec, WavTokenizer)

 I am now working on Text to speech, large speech language models, Generative model research. If you are seeking any form of **academic cooperation**, please feel free to email me at shengpengji@zju.edu.cn. I have learned a lot from them. # I'm now looking for full-time positionÔºåwelcome to contact me at shengpengji@zju.edu.cn -->

<!--
I graduate from the Department of Software Engineering in JiLin University (ÂêâÊûóÂ§ßÂ≠¶ËΩØ‰ª∂Â≠¶Èô¢) with a bachelor's degree and continue to study in Zhejiang University (ÊµôÊ±üÂ§ßÂ≠¶ËΩØ‰ª∂Â≠¶Èô¢) with a master's degree now, advised by [Zhou Zhao (ËµµÊ¥≤)](https://person.zju.edu.cn/zhaozhou). I collaborate with [Zhou Long (Âë®Èæô)](https://long-zhou.github.io/), [ShuJie Liu (ÂàòÊ†ëÊù∞)](https://www.microsoft.com/en-us/research/people/shujliu/) from [Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/machine-learning-research-group/) closely. I also collaborate with [Qian chen (ÈôàË∞¶)](https://scholar.google.com.hk/citations?user=8eosmSQAAAAJ&hl=zh-CN), Siqi Zheng (ÈÉëÊñØÂ•á), [Wen Wang (ÁéãÈõØ)](https://scholar.google.com.hk/citations?user=85Tj1OwAAAAJ&hl=zh-CN) from Alibaba Damo Academy Speech Lab. 

My research interest includes speech synthesis, discrete codec, generative model and LLM. I have published some papers (Á¨¨‰∏Ä‰ΩúËÄÖ/ÂÖ±‰∏ÄÁ¨¨‰∏Ä) at the top international AI conferences such as ICLR2025, ACL2025, ACL2024, AAAI2025, ICASSP2024 with total google scholar citations <a href='https://scholar.google.com/citations?user=zuRaB-oAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

I have long aspired to and dedicated myself to producing impactful work, whether through groundbreaking algorithms, foundational research, or pioneering contributions. While I value the publication of papers in prestigious conferences (such as A* conferences or the top three), my primary focus is on the substantive influence and innovation of my research. I am actively seeking long-term academic collaborations and welcome any inquiries. Please feel free to contact me at shengpengji@zju.edu.cn[Êú¨‰∫∫ÈïøÊúüÊúüÂæÖÂπ∂Ëá¥Âäõ‰∫éÂÅöÂá∫ÊúâÂΩ±ÂìçÂäõÁöÑÂ∑•‰ΩúÔºàÊïàÊûúÊÉäËâ≥ÁöÑËêΩÂú∞ÁÆóÊ≥ï/Âü∫Á°ÄÊÄßÁöÑÁ†îÁ©∂/ÂºÄÂàõÊÄßÁöÑÂ∑•‰ΩúÔºâÔºåÂú®ÊÑè‰ΩÜ‰∏çÊòØÂè™ÂÖ≥Ê≥®‰∫éÂèë‰∫ÜÂ§öÂ∞ëÁØáËÆ∫ÊñáÔºàA‰ºö/‰∏âÂ§ß‰ºöÁ≠âÁ≠âÔºâÔºåÁßØÊûÅÂØªÊ±Ç‰∏Ä‰ªΩÈïøÊúüÁöÑÂ≠¶ÊúØÂêà‰Ωú(ÊöëÁ†î/Ëøõ‰øÆ/ÂÆû‰π†)ÔºåÊ¨¢ËøéÂ§ßÂÆ∂ÈöèÊó∂ËÅîÁ≥ªshengpengji@zju.edu.cn]
-->

<!-- # üíª Important! 
ÊúÄËøëÊú¨‰∫∫Âú®ÂØªÊ±Ç‰∏Ä‰ªΩ2024Êò•Â≠£ÔºåÁ†îÁ©∂ÂûãÁÆóÊ≥ïÂÆû‰π†Áîü/Â≠¶ÊúØÂêà‰ΩúÁöÑofferÔºåBase Êù≠Â∑û(ÂÖ∂Ê¨°‰∏äÊµ∑/Âåó‰∫¨/Ê∑±Âú≥)ÔºåÂèØ‰ª•ÈïøÊúüÁ∫ø‰∏ãÂÆû‰π†„ÄÇÁ†îÁ©∂ÊñπÂêëÔºöËØ≠Èü≥ÂêàÊàêÔºåËØ≠Èü≥Â§ßÊ®°ÂûãÁõ∏ÂÖ≥„ÄÇÂØπAIGCÁîüÊàêÂºèÊ®°Âûã/Â§öÊ®°ÊÄÅÁõ∏ÂÖ≥/ÊúâÂæàÁÇ´ÈÖ∑DemoÁöÑÊñπÂêë‰πüÂæàÊÑüÂÖ¥Ë∂£„ÄÇ**Ê∏¥ÊúõÂú®Êñ∞ÁöÑÂÆû‰π†Èò∂ÊÆµÂ≠¶Âà∞‰∏Ä‰∫õÊñ∞ÁöÑ‰∏úË•øÔºåÊåÅÁª≠Âä™ÂäõËøΩÊ±ÇËÉΩÂÅöÂá∫*È´òÂºïÁî®ËÆ∫Êñá*ÁöÑÂ∑•‰Ωú**ÔºåÂú®Ê≠§Âü∫Á°Ä‰πã‰∏äÊúüÂæÖËÉΩÂ∞ÜËÆ∫ÊñáÈÉ®ÁΩ≤ÊàêÂÆûÈôÖÁÆóÊ≥ï‰∫ßÂìÅ„ÄÇËøôÊòØÊàëÁöÑ[‰∏™‰∫∫ÁÆÄÂéÜ](./_pages/cv20240130.pdf)ÔºåÊ¨¢ËøéÂêÑ‰ΩçÈöèÊó∂ËÅîÁ≥ªÊàëÔºåÂæÆ‰ø°ÊâãÊú∫ÂêåÂè∑„ÄÇ -->

<!-- üöß ‚õèÔ∏è üõ†Ô∏è üë∑   -->


# üî• News

<!-- : 2024.08.30 We propose SOTA Speech/Music/Audio Tokenizer [WavTokenizer](https://github.com/jishengpeng/WavTokenizer) [![](https://img.shields.io/github/stars/jishengpeng/WavTokenizer?style=social&label=Code+Stars)](https://github.com/jishengpeng/WavTokenizer) on arxiv üéâüéâ, Huggingface Daily Paper Rank3, Paper With Code Rank3. -->

- *2025.09*: 3 papers are accepted at ICML, NeurIPS, and EMNLP.
- *2025.07*: Language-Codec, WavRAG, T2A-Feedback are selected as ACL Oral. We propose [WavReward](https://arxiv.org/abs/2505.09558) on arxiv. 2 papers are accepted by ICCV and ACM MM.
- *2025.05*: ControlSpeech and Language-Codec are accepted by ACL 2025 Main. 9 papers are accepted by ACL 2025 (8 Main). 1 paper is accepted by ICML 2025.
- *2025.02*: 1 paper is accepted by CVPR 2025. I was selected as a reviewer for ICCV 2025, ACMMM 2025, NeurIPS 2025.
- *2025.01*: üéâüéâ[WavTokenizer](https://github.com/jishengpeng/WavTokenizer) [![](https://img.shields.io/github/stars/jishengpeng/WavTokenizer?style=social&label=Code+Stars)](https://github.com/jishengpeng/WavTokenizer) is accepted by ICLR 2025! 4 papers are accepted by ICLR 2025. I was selected as a reviewer for ARR (ACL 2025) and an outstanding reviewer for ICASSP 2025.
- *2024.12*: DiscreteWM is accepted by AAAI 2025 and I was selected as a reviewer for IJCAI 2025, ICML 2025.
- *2024.11*: We release [WavChat](https://github.com/jishengpeng/WavChat) [![](https://img.shields.io/github/stars/jishengpeng/WavChat?style=social&label=Code+Stars)](https://github.com/jishengpeng/WavChat) (A survey of spoken dialogue models about 60 pages) on arxiv.
- *2024.10*: üéâüéâ I win the Nation Scholarship in the first year of master and was selected as a **Top** Reviewer for NeurIPS 2024. I join Qwen Team this month.
- *2024.09*: 1 paper is accepted by 2024 EMNLP Main.
- *2024.07*: Alibaba Tongyi (co-worker) **open source** a large speech system [![](https://img.shields.io/github/stars/FunAudioLLM/CosyVoice?style=social&label=Code+Stars)](https://github.com/FunAudioLLM/CosyVoice) and release technical report [FunAudioLLM(CosyVoice)](https://arxiv.org/abs/2407.04051) which will largely influence the speech area!
- *2024.07*: 1 paper is accepted by 2024 ACMMM.
- *2024.05*: MobileSpeech is accepted by 2024 ACL Main(Top conference in nlp)!
- *2024.01*: MobileSpeech has been successfully deployed into [Magic6](https://www.hihonor.com/cn/phones/honor-magic6-pro/) series in Honor Mobile phone!
- *2024.01*: MagaTTS 2 (co-worker) is accepted by 2024 ICLR (Top conference in machine learning)!
- *2023.11*: Megatts has been successfully deployed into [products](https://www.volcengine.com/product/voicecloning) at ByteDance<img src='./images/tiktok.png' style='width: 6em;'>!
- *2023.03*: üéâüéâ I join [Microsoft Research Asia(MSRA), Natural Language Computing Group](https://www.microsoft.com/en-us/research/group/natural-language-computing/) <img src='./images/microsoft_logo.svg' style="width: 4em;"> as a research intern!
- *2021.11*: I join [Tsinghua Shenzhen International Graduate School](https://www.sigs.tsinghua.edu.cn/xxxb/list.htm) <img src='./images/qinghua.png' style='width: 6em;'> as a remote intern.

<!--
- *2025.08*: I have been selected as one of Huawei's 'Young Talents Program' (Âçé‰∏∫Â§©ÊâçÂ∞ëÂπ¥).
- *2025.05*: üéâüéâ I join Tencent HunYuan Team as the summer intern under the ‚ÄòProject Up‚Äô (Èùí‰∫ëËÆ°Âàí) Talent Programme. We propose [WavReward](https://arxiv.org/abs/2505.09558) on arxiv.
- *2024.10*: I was selected as a reviewer for CVPR 2025 and AISTATS 2025 (Statistics and Machine Learning).
- *2024.04*: I join <img src='./images/ali.png' style="width: 5em;"> [Alibaba,  DAMO Academy, Tongyi Lab](https://tongyi.aliyun.com/) <img src='./images/tongyi.png' style="width: 4em;"> as a research intern. 
- *2023.12*: TextrolSpeech [![](https://img.shields.io/github/stars/jishengpeng/TextrolSpeech?style=social&label=Code+Stars)](https://github.com/jishengpeng/TextrolSpeech) is accepted by 2024 ICASSP (Top conference in speech)!
- *2023.11*: One Paper (co-worker) is accepted by CCFA IEEE Transactions on Computers.
- *2024.06*: We propose [ControlSpeech](https://github.com/jishengpeng/controlspeech) [![](https://img.shields.io/github/stars/jishengpeng/ControlSpeech?style=social&label=Code+Stars)](https://github.com/jishengpeng/controlspeech) on arxiv.
- *2024.02*: We propose SOTA codec model [Language-Codec](https://arxiv.org/abs/2402.12208) [![](https://img.shields.io/github/stars/jishengpeng/languagecodec?style=social&label=Code+Stars)](https://github.com/jishengpeng/languagecodec) on arxiv.
-->

# üìù Publications(‰∏Ä‰Ωú/ÂÖ±‰∏Ä/È´òÂΩ±ÂìçÂäõ)

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>


*2024.08*: I was selected as a reviewer for ICLR 2025 and ICASSP 2025.
*2024.05*: I was selected as a reviewer for NIPS 2024.
*2024.03*: I was selected as a reviewer for ECCV 2024.
*2024.01*: I was selected as a reviewer for ACM MM 2024.
*2023.08*ÔºöI was selected as a reviewer for EMNLP 2023.
*2022.11*: I join Ping An Technology Company <img src='./images/pingan.png' style='width: 5em;'> as a speech junior algorithm engineer in Shanghai!
*2022.10*: I got the offer of postgraduate study in the School of Software of [Zhejiang University](http://www.cst.zju.edu.cn/). 
*2021.10*: üéâüéâ I win the Nation Scholarship (Top 1%) in the second year of undergraduate! 


- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->
## üéô Controllable and Zero-shot Text-to-Speech, Codec Representation


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images\wavtokenizer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling \\
Authors: **Shengpeng Ji**, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, Rongjie Huang, Yidi Jiang, Qian Chen, Siqi Zheng, Zhou Zhao

- Hugingface Face Daily paper rank3, Our work are promoted by different media and forums, such as Speech Home, and Twitter, which is a Trending Project at both Github and Paperwithcode.
- Audio samples are available in this [website](https://wavtokenizer.github.io/)
- Code is available in this [![](https://img.shields.io/github/stars/jishengpeng/WavTokenizer?style=social&label=Code+Stars)](https://github.com/jishengpeng/WavTokenizer)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='images\textrolspeech1.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models \\
Authors: **Shengpeng Ji**, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao

- Audio samples are available in this [website](https://sall-e.github.io/)
- Code is available in this [![](https://img.shields.io/github/stars/jishengpeng/TextrolSpeech?style=social&label=Code+Stars)](https://github.com/jishengpeng/TextrolSpeech)
</div>
</div>

 <!-- ## üìö Zero-shot TTS -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2024 Main</div><img src='images\mobilespeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech \\
Authors: **Shengpeng Ji**, Ziyue Jiang, Hanting Wang, Jialong Zuo, Zhou Zhao

- Audio samples are available in this [website](https://mobilespeech.github.io/)
- This work was deployed in the Honor phone Magic6 series.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images\watermark.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

DiscreteWM: Speech Watermarking with Discrete Representations \\
Authors: **Shengpeng Ji**, Ziyue Jiang, Jialon Zuo, Minghui Fang, Yifu Chen, Tao Jin, Zhou Zhao

- Audio samples are available in this [website](https://DiscreteWM.github.io/discrete_wm)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025 Main</div><img src='images\controlspeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">


ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control With Decoupled Codec \\
Authors: **Shengpeng Ji**, Jialong Zuo, Minghui Fang, Siqi Zheng, Qian Chen, Wen Wang, Ziyue Jiang, Hai Huang, Xize Cheng, Rongjie Huang, Zhou Zhao 

- Codes are available in this [website](https://github.com/jishengpeng/ControlSpeech) [![](https://img.shields.io/github/stars/jishengpeng/ControlSpeech?style=social&label=Code+Stars)](https://github.com/jishengpeng/ControlSpeech) 
- Audio samples are available in this [website](https://controlspeech.github.io/)
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2025 Oral</div><img src='images\languagecodec.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models \\
Authors: **Shengpeng Ji**, Minghui Fang, Ziyue Jiang, Rongjie Huang, Jialong Zuo, Shulei Wang, Zhou Zhao

- Codes are available in this [website](https://github.com/jishengpeng/languagecodec) [![](https://img.shields.io/github/stars/jishengpeng/languagecodec?style=social&label=Code+Stars)](https://github.com/jishengpeng/languagecodec) 
- Audio samples are available in this [website](https://languagecodec.github.io/)
</div>
</div>



<!--

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">under anonymous reviewing</div><img src='images\VSTTS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

VS-TTS: Controllable Voice Stylization for Text-to-Speech with Natural Language Prompts \\
Authors: Jialung Zuo<sup>*</sup>, Xize Cheng<sup>*</sup>, **Shengpeng Ji<sup>*</sup>**, Ziyue Jiang, Minghui Fang, Zhiqing Hong, Rongjie Huang, Zehan Wang, Tao Jin, Zhou Zhao

- Audio samples are available in this [website](https://vs-tts123.github.io/VSTTS-Demo/)
</div>
</div>

 <div class='paper-box'><div class='paper-box-image'><div><div class="badge">under anonymous reviewing</div><img src='images\languagecodec.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models \\
Authors: **Shengpeng Ji**, Minghui Fang, Ziyue Jiang, Rongjie Huang, Jialong Zuo, Shulei Wang, Zhou Zhao

- Codes are available in this [website](https://github.com/jishengpeng/languagecodec) [![](https://img.shields.io/github/stars/jishengpeng/languagecodec?style=social&label=Code+Stars)](https://github.com/jishengpeng/languagecodec) 
- Audio samples are available in this [website](https://languagecodec.github.io/)
</div>
</div> -->

``ACL 2025 Oral`` [WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models](), **Yifu Chen<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, Haoxiao Wang, Ziqing Wang, Siyu Chen, Jinzheng He, Jin Xu, Zhou Zhao

``ACL 2025 Main`` [UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook](), **Yidi Jiang<sup>*</sup>**, **Qian Chen<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, Yu Xi, Wen Wang, Chong Zhang, Xianghu Yue, ShiLiang Zhang, Haizhou Li



``ACL 2025 Main`` [Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching](), **Jialong Zuo<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, **Minghui Fang<sup>*</sup>**, Mingze Li, Ziyue Jiang, Xize Cheng, Xiaoda Yang, Chen Feiyang, Xinyu Duan, Zhou Zhao

``ACL 2025 Main`` [ACE: A Generative Cross-Modal Retrieval Framework With Coarse-To-Fine Semantic Modeling](), **Minghui Fang<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, **Jialong Zuo<sup>*</sup>**, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu, Gang Wang, Zhenhua Dong, Zhou Zhao


``ICASSP 2025`` [PFlow-VC: Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned Flow Matching Model](), **Jialong Zuo<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, Minghui Fang, Ziyue Jiang, Xize Cheng, Qian Yang,Wenrui Liu, Guangyan Zhang, Zehai Tu, Yiwen Guo, Zhou Zhao

``EMNLP 2025 Findings`` [InteractSpeech: A Speech Dialogue Interaction Corpus for Spoken Dialogue Model](), **Yifu Chen<sup>*</sup>**, **Shengpeng Ji<sup>*</sup>**, Ziqing Wang, Hanting Wang, Zhou Zhao

 ``Alibaba Technical report`` [FunAudioLLM: Voice Understanding and Generation Foundation Models for Natural Interaction Between Humans and LLMs](https://arxiv.org/abs/2407.04051), Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, **Shengpeng Ji**, Yabin Li, Zerui Li, Heng Lu, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song, Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie Yan, Yexin Yang, Bin Zhang, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Siqi Zheng


 ``ICLR 2024 (Zero-shot TTS)`` [MegaTTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](), Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, **Shengpeng Ji**, Chen Zhang, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun MA, Zhou Zhao
 




<!--
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">DiffBeautifier</div><img src='images/DiffBeautifier.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

DiffBeautifier:Fast Diffusion Model for High-Fidelity Singing Voice Beautifying \\
Authors: **Shengpeng Ji**, Xulong Zhang, Jianzong Wang, Ziyue Jiang,Jing Xiao


- DiffBeautufier is the first singing voice beautifying model without paired data.
- Audio samples are available in this [website](https://diffbeautifier.github.io/)
</div>
</div>
-->

<!-- # üìù Co-worker Publications -->


<!-- ## üìö Zero-shot TTS
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL ARR12 submit</div><img src='images\mobilespeech.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech \\
Authors: **Shengpeng Ji<sup>*</sup>**, Ziyue Jiang<sup>*</sup>, Hanting Wang, Jialong Zuo, Zhou Zhao

- Audio samples are available in this [website](https://mobilespeech.github.io/)
</div>
</div> -->
<!-- - ``ICLR 2024 (Zero-shot TTS)`` [MegaTTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](), Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, **Shengpeng Ji**, Chen Zhang, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun MA, Zhou Zhao
- ``arxiv (Zero-shot TTS)`` [Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias](), Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, **Shengpeng Ji**, Rongjie Huang, Chunfeng Wang, Xiang Yin, Zejun MA, Zhou Zhao -->

<!-- ## üßë‚Äçüé® Machine Learning -->
<!-- - ``ICML 2024 submit`` [DiscreteWM: Speech Watermarking with Discrete Representations](), Ziyue Jiang*, **Shengpeng Ji<sup>*</sup>**, Yi Ren, Zhenhui Ye, Rongjie Huang, Jinglin Liu, Chen Zhang, Tianyu Pang, Chao Du, Hongcheng Zhu, Zhou Zhao -->
<!-- - ``under anonymous reviewing (Cross-Modal Retrieval)`` [ACE: A Generative Cross-Modal Retrieval Framework With Coarse-To-Fine Semantic Modeling](), Minghui Fang, **Shengpeng Ji**, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu, Gang Wang, Zhenhua Dong, Zhou Zhao
- ``under anonymous reviewing (Speaker Generation)`` [SyncTalklip: Highly Synchronized Lip-Readable Speaker Generation with Multi-Task Learning](), Xiaoda Yang, Xize Cheng, Dongjie Fu, Minghui Fang, Jialung Zuo, **Shengpeng Ji**, Tao Jin, Zhou Zhao
- ``under anonymous reviewing (Multimodal Representation)`` [Unlocking the Potential of Multimodal Unified Representation through Training-Free Codebook Optimization and Hierarchical Event Alignment](), Hai Huang, Yan Xia, **Shengpeng Ji**, Shulei Wang, Hanting Wang, JieMing Zhu, Dongzhen Hua, Zhou Zhao
- ``CCF A IEEE Transactions on Computers (Neural Architecture Search + Network)`` [Generating Neural Networks for Diverse Networking Classification Tasks via Hardware-Aware Neural Architecture Search](), Guorui Xie, Qing Li, Zhenning Shi, Hanbin Fang, **Shengpeng Ji**, Yong Jiang, Zhenhui Yuan, Lianbo Ma, Mingwei Xu
- ``CCF B (Chinese) Computer Science (Application)`` [A Novel College Entrance Filling Recommendation Algorithm Based on Score Line Prediction and Multi-feature Fusion](), Zeqing Wang, **Shengpeng Ji**, Xin Li, Zixuan Zhao, Pengxu Wang, Xiaosong Han -->

# üéñ Honors and Awards
- *2024.10* National Scholarship (master) (Top 1%, 2/327)
- *2023.06* Outstanding graduate of Jilin University (Top 5%)
- *2023.06* One-class scholarship of Jilin University (Top 1%, 1/392)
- *2021.10* National Scholarship (Undergraduate) (Top 1%, 5/392)

# üìñ Educations
- *2023.09 - 2026.03*, Master, Software Engineering, Zhejiang University.
- *2019.09 - 2023.06*, Undergraduate, Software Engineering, JiLin Univeristy.

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->
# üßë‚Äçüé® Professional Services
Conference Reviewer/Program Committee: EMNLP 2023, ACM-MM 2024, ECCV 2024, NeurIPS 2024 (outstanding reviewer), ICASSP 2025 (outstanding reviewer), AISTATS 2025, ICLR 2025, CVPR 2025, IJCAI 2025, ICML 2025, ACL 2025 (ARR 2), ICCV 2025, NeurIPS 2025, ACM-MM 2025, EMNLP 2025, AAAI 2026.


# üíª Internships
- *2025.05 - now*, [Tencent, HunYuan Team](https://hunyuan.tencent.com/) , Èùí‰∫ëËÆ°Âàí, Shanghai Xuhui Area.
- *2024.10 - 2025.05*, [Alibaba, Qwen Team](https://tongyi.aliyun.com/) <img src='./images/ali.png' style="width: 5em;">, Hangzhou YunGu Area.
- *2024.04 - 2024.10*, [Alibaba, DAMO Academy, Tongyi Lab](https://tongyi.aliyun.com/) <img src='./images/ali.png' style="width: 5em;">, Hangzhou YunGu Area.
- *2023.03 - 2023.08*, [MSRA,Natural Language Computing Group](https://www.microsoft.com/en-us/research/group/natural-language-computing/) <img src='./images/microsoft_logo.svg' style="width: 4em;">, Beijing HaiDian Area.
- *2021.11 - 2022.05*, [Tsinghua Shenzhen International Graduate School](https://www.sigs.tsinghua.edu.cn/xxxb/list.htm) <img src='./images/qinghua.png' style='width: 6em;'>, Remote.
<!--
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=54e0ojatafc&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> -->
<br />
<br />
<br />
<br />
<br />
<div style="text-align: center; line-height: 100px">
<a href='https://clustrmaps.com/site/1c2mn'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=080808&w=590&t=tt&d=9ho8v9ULH9xn-2TksQGHsXGbF0_TWVbqlTuc_kmS7Ls&co=ffffff&ct=808080'/></a>
</div>
